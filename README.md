# NeuralNetwork

This folder consists of all the basic functions of Neural Network like Activation Functions, Loss value and types of Gradient Descents.
The basic neural networks are implemented using scratch.
Below are the different notebooks used in this folder.
- Activation Functions
- Stochastic_Batch_and_Mini_Batch_Gradient_Descent
- Loss_Functions
- Neural_Network_From_Scratch

#### 1. Activation Functions
This notebook consists of the basic activation functions used inside the neural network like sigmoid, ReLU and LeakyRelu written as simple functions.

<p align="center">
  <img width="460" height="200" src="https://editor.analyticsvidhya.com/uploads/49097ACT.gif">
</p>

#### 2. Stochastic_Batch_and_Mini_Batch_Gradient_Descent
This notebook consists of the Gradient Descent functions from scratch and explains on how each of it works and the difference between each of these categories.

<p align="center">
  <img width="460" height="300" src="https://metamug.com/article/images/gradient-descent.png">
</p>

#### 3. Loss_Functions
This notebook consists of all the loss functions like Mean Absolute Error, Mean Square Error and Binary Cross Entropy Error.


<p align="center">
  <img width="460" height="300" src="https://miro.medium.com/max/1232/1*N1PyOYeog-vyytRbwEwQCQ.png">
</p>

#### 4. Neural_Network_From_Scratch
This notebook consists of a neural network which has been created from scratch using the python and not keras libraries. A comparision has been made between the inbuilt on and the one created from scratch.

<p align="center">
  <img width="460" height="350" src="https://tikz.net/wp-content/uploads/2021/12/neural_networks-001.png">
</p>
